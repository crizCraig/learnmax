- ~~Pass the action in as an embedding~~
  - Learn new state embedding so that the embedding can change to adapt with the added position embedding
  - ~~Understand why recent ViT models don't use the position encoding~~ (they just use conv for position)
  - ~~Try summing it with the state just like the position embedding~~
- ~~since loss is not decreasing, we should try to overfit on a single batch~~
- ~~visualize actual states and predicted states. right now we are not searching through a tree, so we should just be playing random actions and predicting the latent state results.~~
- ~~integrate transformer to take actions~~
- Resume accurate 1 step model, predict most likely state-action trajectories, and visualize them
- ~~See if multistep prediction problem is due to forwarding outside of training~~
- Count state visits
- ~~Reverse entropy to plan along well understood paths~~
- So I accidentally trained GPT from a randomized DVQ and was able to achieve high prediction accuracy. This suggests we can train them together or not train the DVQ at all to reconstruct...
- Try predicting action-states with sequences of image-patch-tokens, instead of or in addition to image-tokens as this should allow for better generalization / translation invariance. This as it enables the position encoding to learn 2D relationships vs now where the position encoding just represents a single dimensional frame number. An alternative easier solution would be to add a fully connected layer to the top of the DVQ before the quantization that learns 2D => 1D that can reconstruct. This hopefully will help build a 1D representation that conveys some translation invariance and other generalization which the transformer can use. Basically, spatial abstraction is already achievable in CNNs (e.g. classification). Pixel / feature prediction and self-supervised learning could also be helpful (or any supervised task that you have labels for) for creating a better spatial abstraction which can then be tokenized/discretized and used in GPT for temporal abstraction. Discretization seems to be critical for temporal abstraction in that it makes the number of predicted possibilities tractable for prediction in search trees.
- Rather than weight entropy by saliency level to encourage more abstract plans, perhaps the weighting should just be based on the total entropy expected divided by the expected duration of the abstract plan cf [Semi-MDP](http://user.engineering.uiowa.edu/~dbricker/Stacks_pdf1/SMDP_Intro.pdf).
- We can predict latent states for a single action, go-right, within a couple hours of training. Also, we have many aliased states (see [50-neighbor-knn](results/first_100_clusters_50_knn.PNG)). Finally we have many action aliases, e.g. right-up and right, noop and up, etc... So the 70k softmax at the end of GPT is 50 to 100x bigger than we need it.
- We should add extra states to the transformer softmax by using kmeans to see when we can increase k while maintaining a good centroid separation. Avg distance to centroid will decrease with larger k, but distance between centroids will usually decrease. We want to find times when this decrease is minimal, suggesting the new cluster is well separated from the others. The experiences that go into the above clustering should be obtained both externally and internally. By internally, I mean through some generative process (i.e. GAN, diffusion model) that produces novel token representations (e.g. humans living on mars, or a huge snake with wings) and attempts to integrate these representations into a plan. It's unclear how to best find which generated tokens will lead to the most learning, but it should be driven by things like, "humans on mars will avoid extincition and therefore provide many more opportunities for learning in the long-term" or "huge snakes with wings would be cool/scary and drawing them would allow sharing the idea with others hence increasing learning in a group of people"
- Language
- Use pin_memory in dataloader when using disk-backed replay buffer
- Allow evaluating hypotheticals within externally unvisted states. Most likely this happens by creating states internally and simulating several of those. Also, we should be able to tell when a low level plan, like reading a book, does not affect a high level plan, like taking a ship to Mars. In this case, the high level context (riding to Mars) should not change the prediction at a low level that reading a book is unlikely to physically change much around us. We don't even need to plan and simulate the result of reading a book while riding to Mars. The upper level context simply doesn't carry enough weight with regard to the lower level that we need to consider it affecting the lower level. I.e. it doesn't matter if I read the book at home or on the way to Mars, the outcome is basically the same. However, if I hammer a nail into the wall, it will have a drastically different effect. This type of reasoning is not yet handled by the learnmax model that I can think of. It should be able to forward simulate that puncturing the ship is much more dangerous than puncturing a wall. I suppose a language model could try to query these types of plans and relate them to the sensorimotor model... Granted puncturing any vehicle is unusual. The context could also be like that of a language model, in that it just comes from a previous token instead of from some higher level model. So internally you'd have the state for
 riding a ship to mars, followed by reading a book, and then subsequent tokens would signal that this is safe due to internal simulations from reading a book in different contexts and integration with a language model that can glean this from others' experience. 

Notes:
#### Comparison with hierarchy of abstract machines (HAMs)
These have discrete action, call, choice, and stop where execution is occurring at one level of abstraction at a time (i.e. sequentially). A big difference between this and learnmax is that learnmax can search within different levels of abstraction in parallel. Since high level plans don't change as often, most searching is done in the lower levels even when executing a high level plan to find some long term entropy. So your basically optimizing for entropy reduction per unit time. However since high level entropy possibly unlocks new vistas and worlds of low level entropy, we still should perhaps afford more weight to high level entropy just based on level alone. 

#### Comparison with options in hierarchical RL
It seems that options may support planning at multiple levels simultaneously. However, I don't see a way to automatically learn the options. Rather they are provided by humans. 
